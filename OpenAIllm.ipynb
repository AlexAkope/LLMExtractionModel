{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2654e-4591-46e2-b7ec-97bc20075c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fetching fallback ticker->CIK map from SEC (may take a moment).\n",
      "\n",
      "=== Processing 3M (MMM), CIK=66740 ===\n",
      "[get_8k_filings] Checking feed URL => https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=66740&type=8-K&count=20&output=atom\n",
      "  - Entry #1: date=2025-03-13, detail page => https://www.sec.gov/Archives/edgar/data/66740/000110465925023448/0001104659-25-023448-index.htm\n",
      "  - Entry #2: date=2025-02-26, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674025000019/0000066740-25-000019-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674025000019/a22625-8xkexhibit.htm for filing date=2025-02-26\n",
      "  - Entry #3: date=2025-02-21, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674025000015/0000066740-25-000015-index.htm\n",
      "  - Entry #4: date=2025-02-10, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674025000012/0000066740-25-000012-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674025000012/pressrelease2-10x25.htm for filing date=2025-02-10\n",
      "  - Entry #5: date=2025-01-21, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674025000002/0000066740-25-000002-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674025000002/q42024-8kerexx991.htm for filing date=2025-01-21\n",
      "  - Entry #6: date=2024-10-22, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000098/0000066740-24-000098-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000098/q32024-8kerexx991.htm for filing date=2024-10-22\n",
      "  - Entry #7: date=2024-08-20, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000091/0000066740-24-000091-index.htm\n",
      "  - Entry #8: date=2024-08-01, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000088/0000066740-24-000088-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000088/amaheshwariofferletter0726.htm for filing date=2024-08-01\n",
      "  - Entry #9: date=2024-07-26, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000077/0000066740-24-000077-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000077/q22024-8kerexx991.htm for filing date=2024-07-26\n",
      "  - Entry #10: date=2024-07-10, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000070/0000066740-24-000070-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000070/pressreleasejuly10.htm for filing date=2024-07-10\n",
      "  - Entry #11: date=2024-06-13, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000064/0000066740-24-000064-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000064/ex991-projectmercurynewsre.htm for filing date=2024-06-13\n",
      "  - Entry #12: date=2024-05-16, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000058/0000066740-24-000058-index.htm\n",
      "  - Entry #13: date=2024-04-30, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000051/0000066740-24-000051-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000051/q12024-8kerexx991.htm for filing date=2024-04-30\n",
      "  - Entry #14: date=2024-04-04, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000044/0000066740-24-000044-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000044/pro-forma8kexx991.htm for filing date=2024-04-04\n",
      "  - Entry #15: date=2024-04-04, detail page => https://www.sec.gov/Archives/edgar/data/66740/000162828024014795/0001628280-24-014795-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000162828024014795/exhibit991-8xk.htm for filing date=2024-04-04\n",
      "  - Entry #16: date=2024-03-12, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000029/0000066740-24-000029-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000029/pressrelease31224.htm for filing date=2024-03-12\n",
      "  - Entry #17: date=2024-03-08, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000023/0000066740-24-000023-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000023/pressrelease3-8x24.htm for filing date=2024-03-08\n",
      "  - Entry #18: date=2024-02-27, detail page => https://www.sec.gov/Archives/edgar/data/66740/000114036124009810/0001140361-24-009810-index.htm\n",
      "  - Entry #19: date=2024-01-29, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000007/0000066740-24-000007-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000007/a129248kex-991.htm for filing date=2024-01-29\n",
      "  - Entry #20: date=2024-01-23, detail page => https://www.sec.gov/Archives/edgar/data/66740/000006674024000005/0000066740-24-000005-index.htm\n",
      "[get_8k_filings] Found EX-99.1 doc: https://www.sec.gov/Archives/edgar/data/66740/000006674024000005/a2023q4-8kerexx991.htm for filing date=2024-01-23\n",
      "  -> Checking filing dated 2025-02-26, doc link => https://www.sec.gov/Archives/edgar/data/66740/000006674025000019/a22625-8xkexhibit.htm\n",
      "[fetch_filing_document] Downloading EX-99.1 => https://www.sec.gov/Archives/edgar/data/66740/000006674025000019/a22625-8xkexhibit.htm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import openai\n",
    "\n",
    "# ================== CONFIG OPTIONS ==================\n",
    "# These define essential constants and user-configurable settings:\n",
    "NEW_PRODUCT_KEYWORDS = [\n",
    "    \"new product\", \"launch\", \"introduced\", \"release\", \n",
    "    \"service\", \"technology\", \"solution\", \"innovation\",\n",
    "    \"epyc\"\n",
    "]\n",
    "MAX_TEXT_FOR_LLM = 4000         # Maximum number of characters from each document to send to the LLM\n",
    "SEC_REQUEST_TIMEOUT = 60        # Number of seconds to wait for SEC server responses\n",
    "PRINT_LLM_RESPONSE = True       # Whether to print truncated versions of the LLM’s responses for debugging\n",
    "\n",
    "##############################################################################\n",
    "# 1) Read OPENAI_API_KEY from the environment\n",
    "##############################################################################\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# If the user hasn't exported an API key, warn them:\n",
    "if not openai.api_key:\n",
    "    print(\"[WARNING] No OPENAI_API_KEY found. Please export it before running.\")\n",
    "\n",
    "##############################################################################\n",
    "# 2) Function to call ChatGPT using your provided client solution\n",
    "##############################################################################\n",
    "def call_chatgpt_llm(prompt, model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Uses a custom, environment-based OpenAI client.\n",
    "    1. Creates a client (openai.OpenAI) from the environment variable.\n",
    "    2. Calls the 'chat.completions.create' endpoint with the user prompt.\n",
    "    3. Returns the LLM's text content or None if an error occurs.\n",
    "\n",
    "    Arguments:\n",
    "    - prompt       : The text prompt sent to the LLM.\n",
    "    - model        : The name of the LLM model (default: 'gpt-4o-mini').\n",
    "    - max_tokens   : Maximum tokens in the LLM response.\n",
    "    - temperature  : Controls the output randomness. Higher => more creative.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an OpenAI client instance\n",
    "        client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        # Use chat completion endpoint\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        # Return the main text content of the first choice\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        # Print the error and return None if an exception arises\n",
    "        print(\"[call_chatgpt_llm] OpenAI API error:\\n\", e)\n",
    "        return None\n",
    "\n",
    "##############################################################################\n",
    "# 3) Functions for SEC data fetching and processing\n",
    "##############################################################################\n",
    "\n",
    "def fetch_company_tickers(url=\"https://www.sec.gov/files/company_tickers.json\"):\n",
    "    \"\"\"\n",
    "    Downloads a JSON file from the SEC mapping company tickers to their CIK numbers.\n",
    "    Returns a dictionary { TICKER_UPPERCASE: 'CIK_as_string' }.\n",
    "    Used if the local CSV lacks certain tickers or if you want a fallback mechanism.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"MyProductScraper/1.0 (test@example.com)\"}\n",
    "    # Attempt to download the JSON data:\n",
    "    resp = requests.get(url, headers=headers, timeout=SEC_REQUEST_TIMEOUT)\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(\"Error fetching ticker JSON data\")\n",
    "    # Parse the JSON into Python data structure\n",
    "    data = resp.json()\n",
    "\n",
    "    # Map each ticker and CIK to an uppercased dictionary entry\n",
    "    ticker_mapping = {}\n",
    "    for item in data.values():\n",
    "        ticker = item.get(\"ticker\")\n",
    "        cik_str = item.get(\"cik_str\")\n",
    "        if ticker and cik_str:\n",
    "            ticker_mapping[ticker.upper()] = str(cik_str)\n",
    "    return ticker_mapping\n",
    "\n",
    "def get_8k_filings(cik, count=20):\n",
    "    \"\"\"\n",
    "    Retrieves up to 'count' recent 8-K filings from the SEC for a given CIK.\n",
    "    1. Constructs an Atom feed URL.\n",
    "    2. Parses the feed, finds 'entry' tags.\n",
    "    3. For each entry, tries to locate EX-99.1 docs in the detail page table.\n",
    "\n",
    "    Returns a list of dicts: [ { 'filing_date':..., 'filing_url':... }, ... ].\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"MyProductScraper/1.0 (test@example.com)\"}\n",
    "    # Build the SEC feed URL specifying 8-K, the desired company, and entry count\n",
    "    feed_url = (\n",
    "        f\"https://www.sec.gov/cgi-bin/browse-edgar?\"\n",
    "        f\"action=getcompany&CIK={cik}&type=8-K&count={count}&output=atom\"\n",
    "    )\n",
    "    print(f\"[get_8k_filings] Checking feed URL => {feed_url}\")\n",
    "    \n",
    "    # Attempt to download the feed\n",
    "    try:\n",
    "        resp = requests.get(feed_url, headers=headers, timeout=SEC_REQUEST_TIMEOUT)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[get_8k_filings] Request error for {feed_url}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure a successful HTTP status\n",
    "    if resp.status_code != 200:\n",
    "        print(f\"[get_8k_filings] Non-200 status ({resp.status_code}). Returning empty.\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the XML-based response using BeautifulSoup\n",
    "    soup = BeautifulSoup(resp.content, \"lxml-xml\")\n",
    "    entries = soup.find_all(\"entry\")\n",
    "    if not entries:\n",
    "        print(\"[get_8k_filings] No <entry> tags found in Atom feed.\")\n",
    "        return []\n",
    "    \n",
    "    filings = []\n",
    "    for i, entry in enumerate(entries, start=1):\n",
    "        # Attempt to extract the filing date\n",
    "        filing_date_tag = entry.find(\"filing-date\")\n",
    "        filing_date = filing_date_tag.text.strip() if filing_date_tag else None\n",
    "        \n",
    "        # Attempt to locate the index page for that filing\n",
    "        index_url_tag = entry.find(\"filing-href\")\n",
    "        index_url = index_url_tag.text.strip() if index_url_tag else None\n",
    "        \n",
    "        print(f\"  - Entry #{i}: date={filing_date}, detail page => {index_url}\")\n",
    "        if not index_url:\n",
    "            continue\n",
    "        \n",
    "        # Attempt to download the detail page for that filing\n",
    "        try:\n",
    "            detail_resp = requests.get(index_url, headers=headers, timeout=SEC_REQUEST_TIMEOUT)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"    -> detail page error for {index_url}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if detail_resp.status_code != 200:\n",
    "            print(f\"    -> detail page status={detail_resp.status_code}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Parse the HTML to locate a table row referencing EX-99.1\n",
    "        detail_soup = BeautifulSoup(detail_resp.content, \"html.parser\")\n",
    "        product_doc_url = None\n",
    "        table_rows = detail_soup.find_all(\"tr\")\n",
    "        for row in table_rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            if not columns:\n",
    "                continue\n",
    "            # Check if 'ex-99.1' is mentioned in the row text\n",
    "            if any(\"ex-99.1\" in col.get_text(strip=True).lower() for col in columns):\n",
    "                doc_link = row.find(\"a\", href=True)\n",
    "                if doc_link:\n",
    "                    # Construct the full link to the exhibit\n",
    "                    product_doc_url = \"https://www.sec.gov\" + doc_link[\"href\"]\n",
    "                    print(f\"[get_8k_filings] Found EX-99.1 doc: {product_doc_url} for filing date={filing_date}\")\n",
    "                    break\n",
    "        \n",
    "        if product_doc_url:\n",
    "            # Store the relevant doc URL and date in a dictionary\n",
    "            filings.append({\n",
    "                \"filing_date\": filing_date,\n",
    "                \"filing_url\": product_doc_url\n",
    "            })\n",
    "    \n",
    "    return filings\n",
    "\n",
    "def fetch_filing_document(url):\n",
    "    \"\"\"\n",
    "    Downloads the EX-99.1 (or relevant doc) from 'url' and returns it as raw text.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"MyProductScraper/1.0 (test@example.com)\"}\n",
    "    print(f\"[fetch_filing_document] Downloading EX-99.1 => {url}\")\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=SEC_REQUEST_TIMEOUT)\n",
    "    except requests.exceptions.ReadTimeout:\n",
    "        print(\"  -> Timed out while reading data from the SEC server. Skipping this filing.\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  -> Request error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        print(f\"  -> Non-200 status code: {resp.status_code}. Returning None.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the HTML response to plain text\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    return text\n",
    "\n",
    "def extract_new_product_section(text):\n",
    "    \"\"\"\n",
    "    Splits the text by lines, searching for any lines that match NEW_PRODUCT_KEYWORDS.\n",
    "    If none are found, returns the first MAX_TEXT_FOR_LLM chars as a fallback.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    candidate_lines = []\n",
    "    for line in lines:\n",
    "        lower_line = line.lower()\n",
    "        # If any of the product-related keywords appear in the line, keep it\n",
    "        if any(kw in lower_line for kw in NEW_PRODUCT_KEYWORDS):\n",
    "            candidate_lines.append(line.strip())\n",
    "    \n",
    "    if candidate_lines:\n",
    "        combined = \" \".join(candidate_lines)\n",
    "        return combined[:MAX_TEXT_FOR_LLM]\n",
    "    else:\n",
    "        return text[:MAX_TEXT_FOR_LLM]\n",
    "\n",
    "def parse_llm_json(llm_response):\n",
    "    \"\"\"\n",
    "    Locates a JSON object (by searching for the first '{...}' block) in the LLM response string.\n",
    "    Attempts to parse it into a Python dictionary. Returns None on failure.\n",
    "    \"\"\"\n",
    "    if not llm_response:\n",
    "        return None\n",
    "    # Find the first JSON block\n",
    "    match = re.search(r\"\\{.*\\}\", llm_response, re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "    candidate = match.group(0)\n",
    "    try:\n",
    "        return json.loads(candidate)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "def fill_missing_keys(parsed_data, required_keys):\n",
    "    \"\"\"\n",
    "    Ensures that the dictionary 'parsed_data' has all the keys in 'required_keys',\n",
    "    setting them to 'N/A' if absent.\n",
    "    \"\"\"\n",
    "    for key in required_keys:\n",
    "        if key not in parsed_data:\n",
    "            parsed_data[key] = \"N/A\"\n",
    "    return parsed_data\n",
    "\n",
    "def extract_product_info(filing_text, company_name, stock_ticker, max_retries=2):\n",
    "    \"\"\"\n",
    "    Uses the ChatGPT LLM to parse 'filing_text' and extract details about a new product or service.\n",
    "    The LLM is prompted to produce exactly 4 keys in valid JSON:\n",
    "      1) company_name\n",
    "      2) stock_ticker\n",
    "      3) new_product\n",
    "      4) product_description\n",
    "\n",
    "    If no product is found, new_product must be 'N/A'.\n",
    "    \"\"\"\n",
    "    relevant_text = extract_new_product_section(filing_text)\n",
    "    \n",
    "    # Prompt instructing the LLM on the JSON format and what to look for\n",
    "    prompt_template = f\"\"\"You are an assistant that extracts new product or service details from an SEC 8-K filing.\n",
    "You must output exactly four keys in valid JSON (and nothing else): company_name, stock_ticker, new_product, product_description.\n",
    "\n",
    "**New Product or Service**:\n",
    "- Any newly introduced or discussed product, service, technology, solution, or offering in the text.\n",
    "- If the text explicitly says \\\"new\\\", \\\"launched\\\", \\\"introduced\\\", \\\"unveiled\\\", etc., treat that as new_product.\n",
    "- If there's a product name (e.g. \\\"EPYC 9005\\\", \\\"Cloud AI Suite 2.0\\\"), treat it as new_product.\n",
    "- If no product or service is found, set new_product = \\\"N/A\\\".\n",
    "\n",
    "**Product Description**:\n",
    "- If the 8-K has a clear description, use it.\n",
    "- If not, invent a plausible product description based on context.\n",
    "- If new_product = \\\"N/A\\\", set product_description = \\\"N/A\\\".\n",
    "\n",
    "**Output**:\n",
    "- Return only valid JSON with exactly these four keys:\n",
    "  {{\n",
    "    \\\"company_name\\\": \\\"...\\\",\n",
    "    \\\"stock_ticker\\\": \\\"...\\\",\n",
    "    \\\"new_product\\\": \\\"...\\\",\n",
    "    \\\"product_description\\\": \\\"...\\\"\n",
    "  }}\n",
    "- No extra text outside the JSON.\n",
    "\n",
    "Filing text:\n",
    "{relevant_text}\n",
    "\n",
    "Metadata:\n",
    "Company: {company_name}\n",
    "Stock Ticker: {stock_ticker}\n",
    "\n",
    "Return ONLY the JSON.\"\"\"\n",
    "\n",
    "    required_keys = [\"company_name\", \"stock_ticker\", \"new_product\", \"product_description\"]\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Call the LLM with the constructed prompt\n",
    "        llm_response = call_chatgpt_llm(prompt_template, model=\"gpt-4o-mini\", max_tokens=1000, temperature=0.7)\n",
    "        if llm_response is None:\n",
    "            print(f\"[extract_product_info] ChatGPT call returned None for {stock_ticker}, attempt #{attempt+1}\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        if PRINT_LLM_RESPONSE:\n",
    "            print(f\"[extract_product_info] LLM raw response (truncated): {llm_response[:500]}...\")\n",
    "        \n",
    "        # Attempt to parse JSON from the LLM response\n",
    "        data = parse_llm_json(llm_response)\n",
    "        if not isinstance(data, dict):\n",
    "            print(f\"[extract_product_info] LLM didn't return valid JSON for {stock_ticker}, attempt #{attempt+1}\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        # Fill in missing keys with 'N/A'\n",
    "        data = fill_missing_keys(data, required_keys)\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_companies_from_csv(csv_path=\"TICKR&CIK.csv\", output_csv=\"multi_sec_8k_product_releases.csv\"):\n",
    "    \"\"\"\n",
    "    Main pipeline:\n",
    "    1) Reads a CSV containing ticker + CIK data.\n",
    "    2) For each row, fetches up to 20 recent 8-K filings for that CIK.\n",
    "    3) Identifies an EX-99.1 doc from each filing and downloads it.\n",
    "    4) Calls extract_product_info() to parse the doc with ChatGPT.\n",
    "    5) If a product is found, record the information and move on to the next company.\n",
    "    6) Writes all results to a CSV file at the end.\n",
    "    \"\"\"\n",
    "    # Load the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Optionally rename columns to standard ones if the CSV uses different headers\n",
    "    rename_map = {\n",
    "        \"Symbol\": \"Ticker\",\n",
    "        \"Company\": \"Company Name\",\n",
    "        \"CIK Number\": \"CIK\"\n",
    "    }\n",
    "    for old_col, new_col in rename_map.items():\n",
    "        if old_col in df.columns and new_col not in df.columns:\n",
    "            df.rename(columns={old_col: new_col}, inplace=True)\n",
    "    \n",
    "    # Ensure the DataFrame has at least a Ticker column\n",
    "    if \"Ticker\" not in df.columns:\n",
    "        print(\"[ERROR] CSV must contain at least a 'Ticker' column.\")\n",
    "        return\n",
    "    \n",
    "    print(\"[INFO] Fetching fallback ticker->CIK map from SEC (may take a moment).\")\n",
    "    # Fallback mapping if the CSV is missing or incomplete\n",
    "    big_ticker_map = fetch_company_tickers()\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Retrieve ticker and do minor cleaning\n",
    "        ticker = str(row.get(\"Ticker\", \"\")).strip().upper()\n",
    "        if not ticker:\n",
    "            continue\n",
    "        \n",
    "        # Retrieve the company name from the CSV or default to the ticker if blank\n",
    "        company_name = str(row.get(\"Company Name\", ticker)).strip()\n",
    "        \n",
    "        # Attempt to get the CIK from the CSV or fallback mapping\n",
    "        csv_cik = row.get(\"CIK\")\n",
    "        if pd.isna(csv_cik) or not str(csv_cik).strip():\n",
    "            cik = big_ticker_map.get(ticker, \"\")\n",
    "        else:\n",
    "            # Remove trailing .0 if present\n",
    "            try:\n",
    "                cik_int = int(float(csv_cik))\n",
    "                cik = str(cik_int)\n",
    "            except:\n",
    "                cik = str(csv_cik).replace(\".0\", \"\")\n",
    "        \n",
    "        if not cik:\n",
    "            print(f\\\"\\\"\\\"\\n=== {company_name} ({ticker}) => No valid CIK. Skipping.\\\"\\\"\\\")\n",
    "            continue\n",
    "        \n",
    "        print(f\\\"\\\"\\\"\\n=== Processing {company_name} ({ticker}), CIK={cik} ===\\\"\\\"\\\")\n",
    "        # Grab up to 20 of the most recent 8-K filings\n",
    "        filings = get_8k_filings(cik, count=20)\n",
    "        if not filings:\n",
    "            print(f\\\"  -> No EX-99.1 8-K filings found for {ticker} (CIK={cik}).\\\")\n",
    "            continue\n",
    "        \n",
    "        # Sort the filings by date in descending order\n",
    "        def parse_date_str(ds):\n",
    "            try:\n",
    "                return datetime.strptime(ds, \\\"%Y-%m-%d\\\")\n",
    "            except:\n",
    "                return datetime(1900, 1, 1)\n",
    "        filings.sort(key=lambda f: parse_date_str(f[\\\"filing_date\\\"]), reverse=True)\n",
    "        \n",
    "        # Look for a product mention in each filing until one is found\n",
    "        found_product = False\n",
    "        for fdict in filings:\n",
    "            fdate = fdict[\\\"filing_date\\\"]\n",
    "            furl = fdict[\\\"filing_url\\\"]\n",
    "            if not furl:\n",
    "                continue\n",
    "            \n",
    "            print(f\\\"  -> Checking filing dated {fdate}, doc link => {furl}\\\")\n",
    "            # Download the text of EX-99.1\n",
    "            doc_text = fetch_filing_document(furl)\n",
    "            if not doc_text:\n",
    "                print(\\\"     -> EX-99.1 text is empty or None, skipping.\\\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze the text with ChatGPT\n",
    "            info = extract_product_info(doc_text, company_name, ticker)\n",
    "            if info is None:\n",
    "                print(\\\"     -> ChatGPT call not successful. Skipping this 8-K.\\\")\n",
    "                continue\n",
    "            \n",
    "            # If a new product is identified, record and break\n",
    "            if info[\\\"new_product\\\"] != \\\"N/A\\\":\n",
    "                print(f\\\"     => Found product mention => {info['new_product']}\\\")\n",
    "                results.append(info)\n",
    "                found_product = True\n",
    "                break\n",
    "            else:\n",
    "                print(\\\"     -> LLM says 'N/A' => continuing to next 8-K.\\\")\n",
    "            \n",
    "            # Brief sleep to avoid spamming SEC or LLM with too many requests\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if not found_product:\n",
    "            print(f\\\"  -> No new product for {company_name} ({ticker}). (Skipping CSV output)\\\")\n",
    "    \n",
    "    # If any products were found, write them to CSV\n",
    "    if results:\n",
    "        df_out = pd.DataFrame(results)\n",
    "        df_out.to_csv(output_csv, index=False)\n",
    "        print(f\\\"\\\\n[INFO] Found {len(results)} total product mentions. Saved => {output_csv}\\\")\n",
    "    else:\n",
    "        print(\\\"\\\\n[INFO] No product announcements found for any companies. No CSV output.\\\")\n",
    "    \n",
    "    print(\\\"✅ Done! The script has finished processing.\\\")\n",
    "\n",
    "\n",
    "if __name__ == \\\"__main__\\\":\n",
    "    # Run the pipeline with default CSV paths\n",
    "    process_companies_from_csv(\\\"TICKR&CIK.csv\\\", \\\"multi_sec_8k_product_releases.csv\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9a67e-a6a5-42d9-b5ee-d8711eb34ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
